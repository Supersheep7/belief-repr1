{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO7fhwS8rHYR4/m6ks41mM3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# TO DO: Optimize my probes for tensor operations\n","# TO DO: Maybe we could write a training loop for SAPLMA? But we do not want it that way\n","# TO DO: We need some Configs"],"metadata":{"id":"6sawYfmltDWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sF9BFxo_O34g"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.model_selection import KFold, cross_val_score\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import random\n","import torch as t\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader, TensorDataset\n","import pickle\n","\n","'''\n","Here we have the three probes that we will deploy to test for internal representation of belief\n","Logreg is a simple logistic regressor\n","MMP is a mass-mean probes as described in Marks & Tegmark 2023\n","Neural is a tentative copy of SAPLMA as described in Azaria & Mitchell 2023\n","'''\n","\n","random.seed(42)\n","\n","class LogReg():\n","\n","    def __init__(self):\n","        self.layers = [x for x in range(layers)]\n","        self.data = None\n","        self.gold = None\n","        self.probe = LogisticRegression()\n","\n","    def fit(data, gold, self):\n","        self.probe.fit(data, gold)\n","\n","    def cross_validation(self, X, y, cv=5, scoring='accuracy'):             # To test\n","        kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n","        scores = cross_val_score(self.fit(), X, y, cv=kf, scoring=scoring)\n","        return scores\n","\n","    def predict(data, self):\n","        self.probe.predict(data)\n","\n","    def save(llm, layer, self):\n","        with open(f'logreg_{llm}_{layer}.pkl', 'wb') as file:\n","            pickle.dump(self, file)\n","\n","class Mmp():\n","\n","    def __init__(self, layers):\n","        self.layers = [x for x in range(layers)]\n","        self.data = None\n","        self.gold = None\n","        self.probe = LogisticRegression()\n","        self.lda = LDA(n_components=2)\n","\n","    def cross_validation(self, X, y, cv=5, scoring='accuracy'):             # To test\n","        kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n","        scores = cross_val_score(self.fit(), X, y, cv=kf, scoring=scoring)\n","        return scores\n","\n","    def fit(data, gold, self):\n","        data = self.lda.fit_transform(data, gold)\n","        self.probe.fit(data, gold)\n","\n","    def predict(data, self):\n","        data = self.lda.transform(data)\n","        self.probe.predict(data)\n","\n","    def save(llm, layer, self):\n","        with open(f'mmp_{llm}_{layer}.pkl', 'wb') as file:\n","            pickle.dump(self.probe, file)\n","\n","class Neural(nn.Module):\n","\n","    # Init\n","\n","    def __init__(self, input_dim, hidden_dim=256, hidden_dim2=128, hidden_dim3=64, output_dim=1, threshold=0.5):\n","        super(Neural, self).__init__()\n","\n","        # Architecture\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim2)\n","        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n","        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n","        self.dropout = nn.Dropout(p=0.2)\n","        self.criterion = nn.BCELoss()\n","\n","        # Hyperparameters\n","\n","        self._initialize_weights()\n","        self.device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.optimizer = Adam()\n","        self.threshold = threshold\n","        self.best = None\n","\n","        # Data\n","        self.input_dim = input_dim\n","        self.llm = \"Default\"\n","\n","    def _initialize_weights(self):\n","        nn.init.xavier_uniform_(self.fc1.weight)\n","        nn.init.zeros_(self.fc1.bias)\n","        nn.init.xavier_uniform_(self.fc2.weight)\n","        nn.init.zeros_(self.fc2.bias)\n","        nn.init.xavier_uniform_(self.fc3.weight)\n","        nn.init.zeros_(self.fc3.bias)\n","        nn.init.xavier_uniform_(self.fc4.weight)\n","        nn.init.zeros_(self.fc4.bias)\n","\n","    # The following two functions will be useful to pickle the probes with meaningful filenames\n","\n","    def set_layers(new_layers, self):\n","        self.layers = new_layers\n","\n","    def set_llm(new_llm, self):\n","        self.llm = new_llm\n","\n","    def forward(self, data, train=False):\n","\n","        stream = nn.Flatten()(data) # Data should be passed through dataloader\n","        out = self.fc1(stream)\n","        out = self.dropout(out)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        out = self.dropout(out)\n","        out = self.relu(out)\n","        logits = self.fc3(out)\n","        probs = self.sigmoid(logits)\n","        preds = (probs >= self.threshold).float()\n","\n","        if train:\n","            return probs    # We want to calculate loss\n","        else:\n","            return preds\n","\n","    def save(self, llm, layer):\n","        with open(f'neural_{llm}_{layer}.pkl', 'wb') as file:\n","            pickle.dump(self, file)\n","\n","''' The following code is from https://github.com/collin-burns/discovering_latent_knowledge/blob/main/CCS.ipynb by Burns et al. 2022 '''\n","\n","\n","class MLPProbe(nn.Module):\n","    def __init__(self, d):\n","        super().__init__()\n","        self.linear1 = nn.Linear(d, 100)\n","        self.linear2 = nn.Linear(100, 1)\n","\n","    def forward(self, x):\n","        h = F.relu(self.linear1(x))\n","        o = self.linear2(h)\n","        return torch.sigmoid(o)\n","\n","class CCS(object):\n","    def __init__(self, x0, x1, nepochs=1000, ntries=10, lr=1e-3, batch_size=-1,\n","                 verbose=False, device=\"cuda\", linear=True, weight_decay=0.01, var_normalize=False):\n","        # data\n","        self.var_normalize = var_normalize\n","        self.x0 = self.normalize(x0)\n","        self.x1 = self.normalize(x1)\n","        self.d = self.x0.shape[-1]\n","\n","        # training\n","        self.nepochs = nepochs\n","        self.ntries = ntries\n","        self.lr = lr\n","        self.verbose = verbose\n","        self.device = device\n","        self.batch_size = batch_size\n","        self.weight_decay = weight_decay\n","\n","        # probe\n","        self.linear = linear\n","        self.initialize_probe()\n","        self.best_probe = copy.deepcopy(self.probe)\n","\n","\n","    def initialize_probe(self):\n","        if self.linear:\n","            self.probe = nn.Sequential(nn.Linear(self.d, 1), nn.Sigmoid())\n","        else:\n","            self.probe = MLPProbe(self.d)\n","        self.probe.to(self.device)\n","\n","    def normalize(self, x):\n","        \"\"\"\n","        Mean-normalizes the data x (of shape (n, d))\n","        If self.var_normalize, also divides by the standard deviation\n","        \"\"\"\n","        normalized_x = x - x.mean(axis=0, keepdims=True)\n","        if self.var_normalize:\n","            normalized_x /= normalized_x.std(axis=0, keepdims=True)\n","\n","        return normalized_x\n","\n","    def get_tensor_data(self):\n","        \"\"\"\n","        Returns x0, x1 as appropriate tensors (rather than np arrays)\n","        \"\"\"\n","        x0 = torch.tensor(self.x0, dtype=torch.float, requires_grad=False, device=self.device)\n","        x1 = torch.tensor(self.x1, dtype=torch.float, requires_grad=False, device=self.device)\n","        return x0, x1\n","\n","    def get_loss(self, p0, p1):\n","        \"\"\"\n","        Returns the CCS loss for two probabilities each of shape (n,1) or (n,)\n","        \"\"\"\n","        informative_loss = (torch.min(p0, p1)**2).mean(0)\n","        consistent_loss = ((p0 - (1-p1))**2).mean(0)\n","        return informative_loss + consistent_loss\n","\n","    def get_acc(self, x0_test, x1_test, y_test):\n","        \"\"\"\n","        Computes accuracy for the current parameters on the given test inputs\n","        \"\"\"\n","        x0 = torch.tensor(self.normalize(x0_test), dtype=torch.float, requires_grad=False, device=self.device)\n","        x1 = torch.tensor(self.normalize(x1_test), dtype=torch.float, requires_grad=False, device=self.device)\n","        with torch.no_grad():\n","            p0, p1 = self.best_probe(x0), self.best_probe(x1)\n","        avg_confidence = 0.5*(p0 + (1-p1))\n","        predictions = (avg_confidence.detach().cpu().numpy() < 0.5).astype(int)[:, 0]\n","        acc = (predictions == y_test).mean()\n","        acc = max(acc, 1 - acc)\n","\n","        return acc\n","\n","    def train(self):\n","        \"\"\"\n","        Does a single training run of nepochs epochs\n","        \"\"\"\n","        x0, x1 = self.get_tensor_data()\n","        permutation = torch.randperm(len(x0))\n","        x0, x1 = x0[permutation], x1[permutation]\n","\n","        # set up optimizer\n","        optimizer = torch.optim.AdamW(self.probe.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n","\n","        batch_size = len(x0) if self.batch_size == -1 else self.batch_size\n","        nbatches = len(x0) // batch_size\n","\n","        # Start training (full batch)\n","        for epoch in range(self.nepochs):\n","            for j in range(nbatches):\n","                x0_batch = x0[j*batch_size:(j+1)*batch_size]\n","                x1_batch = x1[j*batch_size:(j+1)*batch_size]\n","\n","                # probe\n","                p0, p1 = self.probe(x0_batch), self.probe(x1_batch)\n","\n","                # get the corresponding loss\n","                loss = self.get_loss(p0, p1)\n","\n","                # update the parameters\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","        return loss.detach().cpu().item()\n","\n","    def repeated_train(self):\n","        best_loss = np.inf\n","        for train_num in range(self.ntries):\n","            self.initialize_probe()\n","            loss = self.train()\n","            if loss < best_loss:\n","                self.best_probe = copy.deepcopy(self.probe)\n","                best_loss = loss\n","\n","        return best_loss\n"]}]}